{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsk2327/DistAya/blob/main/KLDBasedPruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets\n",
        "# I ran this on colab, so all the required dependencies were already installed. On a another machine, you'll probably need to install all the dependencies by hand."
      ],
      "metadata": {
        "id": "8iM8w3Es7xB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git huggingface_hub"
      ],
      "metadata": {
        "id": "bfxBw1e_QI9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "w9SfLooEQpAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "JvH1PiAlQcpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download CohereForAI/aya-23-8B --repo-type model --local-dir aya-23-8B"
      ],
      "metadata": {
        "id": "Bu-1mOQoR8Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download CohereForAI/aya_dataset --repo-type dataset --local-dir aya-dataset"
      ],
      "metadata": {
        "id": "j8QLtmDlQuwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def getmodule(module: torch.nn.Module, target_module: str):\n",
        "    \"\"\"Get a target module from a given module.\"\"\"\n",
        "    submodules = target_module.split(\".\", 1)\n",
        "    if submodules[0].isdigit():\n",
        "      next_module = module[int(submodules[0])]\n",
        "    else:\n",
        "      next_module = getattr(module, submodules[0])\n",
        "    if len(submodules) == 1:\n",
        "        return next_module\n",
        "    return getmodule(next_module, submodules[-1])\n",
        "\n",
        "def setmodule(module: torch.nn.Module, target_module: str, value: torch.nn.Module):\n",
        "    \"\"\"Set a target module in a given module.\"\"\"\n",
        "    submodules = target_module.split(\".\", 1)\n",
        "    if len(submodules) == 1:\n",
        "        if submodules[0].isdigit():\n",
        "            module[int(submodules[0])] = value\n",
        "        else:\n",
        "            setattr(module, submodules[0], value)\n",
        "    else:\n",
        "        setmodule(getattr(module, submodules[0]), submodules[-1], value)"
      ],
      "metadata": {
        "id": "45K0DmqyQAZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class IndexDataset(Dataset):\n",
        "    def __init__(self, tensors):\n",
        "        self.tensors = tensors\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.tensors[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tensors)\n",
        "\n",
        "# Loading the data and tokenizing it\n",
        "def process_data(samples, tokenizer, seq_len, field_name):\n",
        "    test_ids = tokenizer(\"\\n\\n\".join(samples[field_name]), return_tensors='pt').input_ids[0]\n",
        "    test_ids_batch = []\n",
        "    nsamples = test_ids.numel() // seq_len\n",
        "\n",
        "    for i in range(nsamples):\n",
        "        batch = test_ids[(i * seq_len):((i + 1) * seq_len)]\n",
        "        test_ids_batch.append(batch)\n",
        "    test_ids_batch = torch.stack(test_ids_batch)\n",
        "    return IndexDataset(tensors=test_ids_batch)\n",
        "\n",
        "def merge_instructions(sample):\n",
        "    return {\"text\": \"\\n\\n\".join([sample[\"inputs\"], sample[\"targets\"]])}\n",
        "\n",
        "def get_aya_loaders(tokenizer, seq_len=512, batch_size=4, max_samples=256):\n",
        "    test_data = load_dataset('/content/aya-dataset/', 'default', split='test')\n",
        "    test_data = test_data.map(merge_instructions,\n",
        "                              batched=False,\n",
        "                              remove_columns=test_data.column_names)\n",
        "\n",
        "    if max_samples is not None:\n",
        "        test_data = test_data.select(range(max_samples)) # select a small subset just for testing\n",
        "    test_dataset = process_data(test_data, tokenizer, seq_len, 'text')\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return test_loader\n",
        "\n",
        "def get_wikitext_loaders(tokenizer, seq_len=128, batch_size = 4, max_samples=256):\n",
        "    test_data = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "    test_data = test_data.shuffle(seed=42)\n",
        "    if max_samples is not None:\n",
        "        test_data = test_data.select(range(max_samples)) # select a small subset just for testing\n",
        "    test_dataset = process_data(test_data, tokenizer, seq_len, 'text')\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return test_loader"
      ],
      "metadata": {
        "id": "ycN812LfP8o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from  pathlib import Path\n",
        "import re\n",
        "import logging\n",
        "from argparse import ArgumentParser\n",
        "from pandas import DataFrame\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_layers(llm):\n",
        "    for name, module in llm.named_modules():\n",
        "        if re.search(\"layers\\.\\d+$\", name):\n",
        "            yield name, module\n",
        "\n",
        "def set_pruned_layers(llm):\n",
        "    for name, module in get_layers(llm):\n",
        "        pruned_layer = PruneLayer(module, drop=False, is_last=False)\n",
        "        setmodule(llm, name, pruned_layer)\n",
        "\n",
        "class PruneLayer(torch.nn.Module):\n",
        "    \"\"\"\"If pruned, the layer will do nothing other than returning its input.\"\"\"\n",
        "    def __init__(self, layer, is_last: bool, drop: bool=False):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "        self.drop = drop\n",
        "        self.is_last = is_last\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, hidden_states, **kwargs):\n",
        "        if self.drop:\n",
        "            return (hidden_states,)\n",
        "        return self.layer(hidden_states, **kwargs)\n",
        "\n",
        "class Sensivity(ABC):\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        set_pruned_layers(llm)\n",
        "\n",
        "    @abstractmethod\n",
        "    def score(self, batch: torch.Tensor, target_module: str) -> float:\n",
        "        \"\"\"PPL or KL-div\"\"\"\n",
        "        pass\n",
        "\n",
        "    def sensivity(self, test_lodaer, target_module):\n",
        "        scores = []\n",
        "        for batch in tqdm(test_lodaer):\n",
        "            batch = batch.to(self.llm.device)\n",
        "            score = self.score(batch, target_module=target_module)\n",
        "            scores.append(score)\n",
        "        return torch.tensor(scores).mean().item()\n",
        "\n",
        "    def prune_layer(self, target_module):\n",
        "        module = getmodule(self.llm, target_module)\n",
        "        module.drop = True\n",
        "\n",
        "    def unprune_layer(self, target_module):\n",
        "        module = getmodule(self.llm, target_module)\n",
        "        module.drop = False\n",
        "\n",
        "    def __call__(self, test_dataset, output_folder):\n",
        "        layers = list(name for name, _ in get_layers(self.llm))\n",
        "        results = []\n",
        "        for name in tqdm(layers):\n",
        "            layer_idx = name.split(\".\")[-1]\n",
        "            sensivity = self.sensivity(test_lodaer=test_dataset, target_module=name)\n",
        "            logging.info(f\"pruned layer={layer_idx}, sensvity={sensivity}\")\n",
        "            results.append({\n",
        "                \"layer\": name,\n",
        "                \"score\": sensivity,\n",
        "            })\n",
        "            print({\n",
        "                \"layer\": name,\n",
        "                \"score\": sensivity,\n",
        "            })\n",
        "            df = DataFrame(results)\n",
        "            df.to_csv(output_folder / f\"sensivities.csv\")\n",
        "\n",
        "class PPLSensivity(Sensivity):\n",
        "    def __init__(self, llm):\n",
        "        super().__init__(llm)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def score(self, batch: torch.Tensor, target_module) -> float:\n",
        "        self.prune_layer(target_module=target_module)\n",
        "        output = self.llm(batch, use_cache=False, output_attentions=False)\n",
        "        lm_logits = output.logits\n",
        "\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        shift_labels = batch[:, 1:].contiguous()\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        self.unprune_layer(target_module=target_module)\n",
        "        return torch.exp(loss).mean().item()\n",
        "\n",
        "class KLDivSensivity(Sensivity):\n",
        "    def __init__(self, llm):\n",
        "        super().__init__(llm)\n",
        "        self.t = 2\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def score(self, batch: torch.Tensor, target_module: str) -> float:\n",
        "        b, *_ = batch.shape\n",
        "        teacher_logits = self.llm(batch, use_cache=False, output_attentions=False).logits\n",
        "        self.prune_layer(target_module=target_module)\n",
        "        student_logits = self.llm(batch, use_cache=False, output_attentions=False).logits\n",
        "\n",
        "        t_probs = F.softmax(teacher_logits / self.t, dim=-1)\n",
        "        s_probs = F.log_softmax(student_logits / self.t, dim=-1)\n",
        "        kl_d = torch.sum(t_probs * (t_probs.log() - s_probs)) / b * (self.t ** 2)\n",
        "\n",
        "        return kl_d.item()\n",
        "\n",
        "def main(args):\n",
        "    output_folder = Path(args.output_folder)\n",
        "    output_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model,\n",
        "                                              trust_remote_code=True)\n",
        "    loader = get_wikitext_loaders if args.data == 'wikitext' else get_aya_loaders\n",
        "    test_loader = loader(tokenizer=tokenizer, max_samples=args.subset, batch_size=args.batch_size)\n",
        "\n",
        "    llm = AutoModelForCausalLM.from_pretrained(args.model,\n",
        "                                               torch_dtype=torch.bfloat16,\n",
        "                                               trust_remote_code=True,\n",
        "                                               device_map=\"auto\")\n",
        "    print(llm)\n",
        "    llm.cuda()\n",
        "    set_pruned_layers(llm)\n",
        "\n",
        "    scorer = PPLSensivity(llm) if args.score == \"perplexity\" else KLDivSensivity(llm)\n",
        "    scorer(test_dataset=test_loader, output_folder=output_folder)"
      ],
      "metadata": {
        "id": "75j6Q-HlPuVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "    output_folder: str = \"sensivities\"\n",
        "    model: str = \"aya-23-8B\"\n",
        "    data: str = \"aya\"\n",
        "    score: str = \"kl_div\"\n",
        "    subset: int = 512\n",
        "    batch_size: int = 2"
      ],
      "metadata": {
        "id": "PhOP0ySGRJ6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Args()\n",
        "main(args)"
      ],
      "metadata": {
        "id": "ObBnMEjWRAad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def sort_by_importance(sensitivity_scores):\n",
        "    layers_scores = []\n",
        "    with open(sensitivity_scores, \"r\") as layer_scores:\n",
        "        next(layer_scores)\n",
        "        for line in layer_scores:\n",
        "            line = line.strip()\n",
        "            *_, layer, score = line.split(\",\")\n",
        "            layers_scores.append((layer, float(score)))\n",
        "\n",
        "    layers, *_ = zip(*sorted(layers_scores, key=lambda x: x[1]))\n",
        "    return list(layers)\n",
        "\n",
        "def prune(llm, sensitivities, reduction: int=50.0):\n",
        "    sorted_layers = sort_by_importance(sensitivity_scores=sensitivities)\n",
        "    num_layers_to_skip = round((reduction * llm.config.num_hidden_layers) / 100)\n",
        "    layers_to_skip = sorted_layers[:num_layers_to_skip + 1]\n",
        "    for layer in layers_to_skip:\n",
        "        delattr(llm.model.layers, layer.split(\".\")[-1])\n",
        "    print(f\"Parameters of the pruned LLM: {llm.num_parameters():,}\")\n",
        "    remaining_layers = sum(1 for _ in llm.model.layers)\n",
        "    llm.config.num_hidden_layers = remaining_layers\n",
        "    llm.save_pretrained(\"aya-4b-kld-pruning\")"
      ],
      "metadata": {
        "id": "01tpqh8xjT7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"aya-23-8B\",\n",
        "                                          trust_remote_code=True)"
      ],
      "metadata": {
        "id": "MHCOxCEDZWSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download yaya-sy/aya-4b-kld-pruning --local-dir aya-4b-kld-pruning"
      ],
      "metadata": {
        "id": "maRTTyUGZZpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AutoModelForCausalLM.from_pretrained(\"aya-23-8B\",\n",
        "                                            torch_dtype=torch.bfloat16,\n",
        "                                            trust_remote_code=True,\n",
        "                                            device_map=\"auto\")"
      ],
      "metadata": {
        "id": "aaCXOikxjcU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune(llm=llm, sensitivities=\"sensivities.csv\")"
      ],
      "metadata": {
        "id": "aLxjK2frnGtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_pruned = AutoModelForCausalLM.from_pretrained(\"aya-4b-kld-pruning\",\n",
        "                                                  torch_dtype=torch.bfloat16,\n",
        "                                                  trust_remote_code=True,\n",
        "                                                  device_map=\"cpu\")"
      ],
      "metadata": {
        "id": "I8qOIvGXodfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_pruned"
      ],
      "metadata": {
        "id": "CnCetECNrlfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"aya-4b-kld-pruning\")"
      ],
      "metadata": {
        "id": "JC2W93NCaTxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_pruned.push_to_hub(\"yaya-sy/aya-4b-kld-pruning-with-tokenizer\")"
      ],
      "metadata": {
        "id": "zwF3h0R9rl5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli upload yaya-sy/aya-4b-kld-pruning aya-4b-kld-pruning"
      ],
      "metadata": {
        "id": "H_ktQ4rprsrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/aya-4b-kld-pruning"
      ],
      "metadata": {
        "id": "y0Eqn8Tobup8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = AutoModelForCausalLM.from_pretrained(\"yaya-sy/aya-4b-kld-pruning\",\n",
        "                                          trust_remote_code=True,\n",
        "                                          device_map=\"auto\")"
      ],
      "metadata": {
        "id": "YlIuESu2am_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"yaya-sy/aya-4b-kld-pruning\",\n",
        "                                          trust_remote_code=True)"
      ],
      "metadata": {
        "id": "epSll3w6b2Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "I9zrNOVJcqYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YyktNChNctJw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}